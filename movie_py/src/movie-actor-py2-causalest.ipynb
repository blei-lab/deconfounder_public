{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(linewidth=60)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import itertools\n",
    "\n",
    "from stanhelper import stanhelper\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as npr\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import invgamma\n",
    "from scipy import sparse, stats\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed:  60382071\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "randseed = 60382071\n",
    "print(\"random seed: \", randseed)\n",
    "random.seed(randseed)\n",
    "np.random.seed(randseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DATA_DIR = '../res/factorfit/'\n",
    "DATA_DIR = '../dat/tmdb_proc/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = pd.read_csv(os.path.join(DATA_DIR, 'train_full.csv'))\n",
    "data = tr_vd_data['rating']\n",
    "row = tr_vd_data['uid']\n",
    "col = tr_vd_data['sid']\n",
    "n_users, n_items = (2828, 901)\n",
    "X = sparse.coo_matrix((data, (row, col)), shape=(n_users, n_items)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yixinwang/py2/lib/python2.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "unique_uid = np.loadtxt(os.path.join(DATA_DIR, 'unique_uid.txt'))\n",
    "user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))\n",
    "tmdb = pd.read_csv(os.path.join(DATA_DIR, \"tmdb_clean.csv\"))\n",
    "tmdbrev = tmdb[['revenue','movie_id', 'budget', \\\n",
    "                'original_language', 'new_movie_id']]\n",
    "tmdbrev['uid'] = [user2id[k] for k in tmdbrev['movie_id']] # map movie_id to u_id in the substitute confounder\n",
    "tmdbrev = tmdbrev.sort_values(['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue = tmdbrev['revenue']\n",
    "y = np.log(revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaU = np.loadtxt(OUT_DATA_DIR + '/cause_pca_k50_U.csv')\n",
    "pmfU = np.loadtxt(OUT_DATA_DIR + '/cause_pmf_k50_U.csv')\n",
    "defU = np.loadtxt(OUT_DATA_DIR + '/cause_def_k_xpost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = tmdb[['budget', 'runtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = preprocessing.StandardScaler().fit(y[:,np.newaxis])\n",
    "y_scaled = y_scaler.fit_transform(y[:,np.newaxis]).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yixinwang/py2/lib/python2.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = X_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaU_scaler = preprocessing.StandardScaler().fit(pcaU)\n",
    "pcaU_scaled = pcaU_scaler.fit_transform(pcaU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmfU_scaler = preprocessing.StandardScaler().fit(pmfU)\n",
    "pmfU_scaled = pmfU_scaler.fit_transform(pmfU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "defU_scaler = preprocessing.StandardScaler().fit(defU)\n",
    "defU_scaled = defU_scaler.fit_transform(defU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_scaler = preprocessing.StandardScaler().fit(cov)\n",
    "cov_scaled = cov_scaler.fit_transform(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, \\\n",
    "cov_train, cov_test, \\\n",
    "pcaU_train, pcaU_test, \\\n",
    "pmfU_train, pmfU_test, \\\n",
    "defU_train, defU_test, \\\n",
    "y_train, y_test, tmdbrev_train, tmdbrev_test = \\\n",
    "    train_test_split(X_scaled, cov_scaled, pcaU_scaled, pmfU_scaled, defU_scaled, \\\n",
    "                     y_scaled, tmdbrev, test_size=0.20, random_state=randseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimate causal effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nctrl_sing = np.zeros(n_items)\n",
    "reg = linear_model.Ridge(alpha=0.0)\n",
    "for i in range(n_items):\n",
    "    reg.fit(X_train[:,i][:,np.newaxis], y_train)\n",
    "    nctrl_sing[i] = reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## control for covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "covctrl_sing = np.zeros(n_items)\n",
    "reg = linear_model.Ridge(alpha=0.0)\n",
    "for i in range(n_items):\n",
    "    reg.fit(np.column_stack([X_train[:,i], cov_train]), y_train)\n",
    "    covctrl_sing[i] = reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## control for PCA substitute confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcactrl_sing = np.zeros(n_items)\n",
    "reg = linear_model.Ridge(alpha=0.0)\n",
    "for i in range(n_items):\n",
    "    reg.fit(np.column_stack([X_train[:,i], pcaU_train]), y_train)\n",
    "    pcactrl_sing[i] = reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## control for PMF substitute confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmfctrl_sing = np.zeros(n_items)\n",
    "reg = linear_model.Ridge(alpha=0.0)\n",
    "for i in range(n_items):\n",
    "    reg.fit(np.column_stack([X_train[:,i], pmfU_train]), y_train)\n",
    "    pmfctrl_sing[i] = reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## control for DEF substitute confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "defctrl_sing = np.zeros(n_items)\n",
    "reg = linear_model.Ridge(alpha=0.0)\n",
    "for i in range(n_items):\n",
    "    reg.fit(np.column_stack([X_train[:,i] - defU_train[:,i]]), y_train)\n",
    "    defctrl_sing[i] = reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## control for PCA substitute confounder and covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcacovctrl_sing = np.zeros(n_items)\n",
    "reg = linear_model.Ridge(alpha=0.0)\n",
    "for i in range(n_items):\n",
    "    reg.fit(np.column_stack([X_train[:,i], pcaU_train, cov_train]), y_train)\n",
    "    pcacovctrl_sing[i] = reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## control for PMF substitute confounder and covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmfcovctrl_sing = np.zeros(n_items)\n",
    "reg = linear_model.Ridge(alpha=0.0)\n",
    "for i in range(n_items):\n",
    "    reg.fit(np.column_stack([X_train[:,i], pmfU_train, cov_train]), y_train)\n",
    "    pmfcovctrl_sing[i] = reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## control for DEF substitute confounder and covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "defcovctrl_sing = np.zeros(n_items)\n",
    "reg = linear_model.Ridge(alpha=0.0)\n",
    "for i in range(n_items):\n",
    "    reg.fit(np.column_stack([X_train[:,i] - defU_train[:,i], cov_train]), y_train)\n",
    "    defcovctrl_sing[i] = reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logpdf(y, ypred, sigma=1.3):\n",
    "    return -0.5 * np.log(2*np.pi) - 0.5 * (y-ypred)**2 / sigma**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nctrllpdfs = logpdf(y_test, np.matmul(np.column_stack([X_test]), nctrl_sing))\n",
    "covctrllpdfs = logpdf(y_test, np.matmul(np.column_stack([X_test]), covctrl_sing))\n",
    "pcactrllpdfs = logpdf(y_test, np.matmul(np.column_stack([X_test]), pcactrl_sing))\n",
    "pmfctrllpdfs = logpdf(y_test, np.matmul(np.column_stack([X_test]), pmfctrl_sing))\n",
    "defctrllpdfs = logpdf(y_test, np.matmul(np.column_stack([X_test]), defctrl_sing))\n",
    "pcacovctrllpdfs = logpdf(y_test, np.matmul(np.column_stack([X_test]), pcacovctrl_sing))\n",
    "pmfcovctrllpdfs = logpdf(y_test, np.matmul(np.column_stack([X_test]), pmfcovctrl_sing))\n",
    "defcovctrllpdfs = logpdf(y_test, np.matmul(np.column_stack([X_test]), defcovctrl_sing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regular test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no control -1.4942115171598733\n",
      "control for X -1.5108222305439594\n",
      "control for PCA confounders -1.2592246089467007\n",
      "control for PMF confounders -1.3087380809092464\n",
      "control for DEF confounders -1.229431034258638\n",
      "control for PCA confounders and X -1.2601227861835826\n",
      "control for PMF confounders and X -1.3167451645857071\n",
      "control for DEF confounders and X -1.2372676433173477\n"
     ]
    }
   ],
   "source": [
    "print(\"no control\", nctrllpdfs.mean())\n",
    "print(\"control for X\", covctrllpdfs.mean())\n",
    "print(\"control for PCA confounders\", pcactrllpdfs.mean())\n",
    "print(\"control for PMF confounders\", pmfctrllpdfs.mean())\n",
    "print(\"control for DEF confounders\", defctrllpdfs.mean())\n",
    "print(\"control for PCA confounders and X\", pcacovctrllpdfs.mean())\n",
    "print(\"control for PMF confounders and X\", pmfcovctrllpdfs.mean())\n",
    "print(\"control for DEF confounders and X\", defcovctrllpdfs.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-English movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_df = pd.read_csv(os.path.join(DATA_DIR, \"spoken_languages_clean.csv\"))\n",
    "spoken_df = spoken_df[[\"new_movie_id\", \"iso_639_1\"]].sort_values(\"new_movie_id\")\n",
    "spoken_dummy = pd.get_dummies(spoken_df[\"iso_639_1\"])\n",
    "spoken_dummy[\"new_movie_id\"] = spoken_df[[\"new_movie_id\"]]\n",
    "spoken_dummy = spoken_dummy.groupby(\"new_movie_id\").sum().reset_index()\n",
    "testset_slang = tmdbrev_test.merge(spoken_dummy, on=\"new_movie_id\")\n",
    "nonenslang = (testset_slang['en']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no control -2.222961562984115\n",
      "control for X -2.254504173940256\n",
      "control for PCA confounders -1.0710504969888004\n",
      "control for PMF confounders -1.1242292842486086\n",
      "control for DEF confounders -0.9962851210880637\n",
      "control for PCA confounders and X -1.0610292404779338\n",
      "control for PMF confounders and X -1.115203922008078\n",
      "control for DEF confounders and X -0.9821726425934653\n"
     ]
    }
   ],
   "source": [
    "print(\"no control\", nctrllpdfs[nonenslang].mean())\n",
    "print(\"control for X\", covctrllpdfs[nonenslang].mean())\n",
    "print(\"control for PCA confounders\", pcactrllpdfs[nonenslang].mean())\n",
    "print(\"control for PMF confounders\", pmfctrllpdfs[nonenslang].mean())\n",
    "print(\"control for DEF confounders\", defctrllpdfs[nonenslang].mean())\n",
    "print(\"control for PCA confounders and X\", pcacovctrllpdfs[nonenslang].mean())\n",
    "print(\"control for PMF confounders and X\", pmfcovctrllpdfs[nonenslang].mean())\n",
    "print(\"control for DEF confounders and X\", defcovctrllpdfs[nonenslang].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-drama/comedy/action movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df = pd.read_csv(os.path.join(DATA_DIR, \"genres_df_clean.csv\"))\n",
    "genres_df = genres_df[[\"new_movie_id\", \"genre\"]].sort_values(\"new_movie_id\")\n",
    "genres_dummy = pd.get_dummies(genres_df[\"genre\"])\n",
    "genres_dummy[\"new_movie_id\"] = genres_df[[\"new_movie_id\"]]\n",
    "genres_dummy = genres_dummy.groupby(\"new_movie_id\").sum().reset_index()\n",
    "testset_genres = tmdbrev_test.merge(genres_dummy, on=\"new_movie_id\")\n",
    "testset_genres[\"unpopular_genres\"] = \\\n",
    "    (1-testset_genres[\"Drama\"])*(1-testset_genres[\"Comedy\"])*(1-testset_genres[\"Action\"])\n",
    "unpop = (testset_genres[\"unpopular_genres\"]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no control -1.825719594297616\n",
      "control for X -1.7568856798104904\n",
      "control for PCA confounders -1.2855852016943037\n",
      "control for PMF confounders -1.345983098440941\n",
      "control for DEF confounders -1.2973511769877217\n",
      "control for PCA confounders and X -1.3125887368141143\n",
      "control for PMF confounders and X -1.3546141406017849\n",
      "control for DEF confounders and X -1.299284688229018\n"
     ]
    }
   ],
   "source": [
    "print(\"no control\", nctrllpdfs[unpop].mean())\n",
    "print(\"control for X\", covctrllpdfs[unpop].mean())\n",
    "print(\"control for PCA confounders\", pcactrllpdfs[unpop].mean())\n",
    "print(\"control for PMF confounders\", pmfctrllpdfs[unpop].mean())\n",
    "print(\"control for DEF confounders\", defctrllpdfs[unpop].mean())\n",
    "print(\"control for PCA confounders and X\", pcacovctrllpdfs[unpop].mean())\n",
    "print(\"control for PMF confounders and X\", pmfcovctrllpdfs[unpop].mean())\n",
    "print(\"control for DEF confounders and X\", defcovctrllpdfs[unpop].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
